{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64b2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import time\n",
    "from random import sample\n",
    "import numpy.random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebc9bc",
   "metadata": {},
   "source": [
    "# GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71c2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, board_dims, start_state, terminal_state, reward_matrix):\n",
    "        self.board_dims = board_dims\n",
    "        self.height, self.width = board_dims\n",
    "        self.start_state = start_state\n",
    "        self.agent_pos = start_state\n",
    "        self.terminal_state = terminal_state\n",
    "        self.reward_matrix = reward_matrix\n",
    "        self.moves = [(-1,0),(0,-1),(1,0),(0,1)] #up,left,down,right\n",
    "        \n",
    "    def get_moves(self, point):\n",
    "        \"\"\"\n",
    "        point - the point our agent is in\n",
    "        This function obtains the points of possible moves you can potentially make from the point provided\n",
    "        \"\"\"\n",
    "        return [tuple(np.array(point)+np.array(x)) for x in self.moves]\n",
    "\n",
    "    def point_to_index(self, point):\n",
    "        \"\"\"\n",
    "        point - the point we want to convert to index (for list of lists e.g.)\n",
    "        This function obtains the index when you iterate across each column over each row of our game board\n",
    "        \"\"\"\n",
    "        return point[0]*self.width + point[1]\n",
    "\n",
    "    def index_to_point(self, index):\n",
    "        \"\"\"\n",
    "        index - the index we want to convert to game square point\n",
    "        This function obtains the index when you iterate across each column over each row of our game board\n",
    "        \"\"\"\n",
    "        #(divisions, remainder)\n",
    "        return (index // self.width, index % self.width)\n",
    "\n",
    "    def get_reward(self,point):\n",
    "        \"\"\"\n",
    "        point - a point in our grid\n",
    "        This function returns the reward for our agent going to the state represented by point\n",
    "        \"\"\"\n",
    "        return self.reward_matrix[point]\n",
    "    \n",
    "    def illegal_move(self,point):\n",
    "        \"\"\"\n",
    "        point - a point that may or may not be in our grid\n",
    "        This function returns true if the move we make takes us off the board and false if the move is legal\n",
    "        \"\"\"\n",
    "        return True if ((point[0] < 0 or point[0] == self.height) or (point[1] < 0 or point[1] == self.width)) else False\n",
    "        \n",
    "    def print_board(self):\n",
    "        print(self.reward_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44011e88",
   "metadata": {},
   "source": [
    "# Individual Game Implementations\n",
    "\n",
    "Here we define each game and have it inherit attributes from GridWorld object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b680af",
   "metadata": {},
   "source": [
    "## Targeting Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1712ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Targeting_Game(GridWorld):\n",
    "    def __init__(self, board_dims, start_state, terminal_state, reward_matrix):\n",
    "        GridWorld.__init__(self,board_dims, start_state, terminal_state, reward_matrix)\n",
    "        \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        agent_pos - tuple point representing agent's current position on game board \n",
    "        remaining_prizes - list of tuples representing remaining prizes and their positions on game board\n",
    "        This function draws our environment\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.new(\"RGB\", (501, 501), \"black\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        w = 500/(self.width)\n",
    "        h = 500/(self.height)\n",
    "        color = \"white\"\n",
    "\n",
    "        #Draw Grid and Start/Stop Squares\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if(i == self.start_state[0] and j == self.start_state[1]):\n",
    "                    color = \"blue\"\n",
    "                elif(i == self.terminal_state[0] and j == self.terminal_state[1]):\n",
    "                    color = \"red\"\n",
    "                draw.rectangle(((0+j*w, 0+i*h),(w+j*w, h+i*h)), outline = \"black\", fill = color)\n",
    "                color =\"white\"\n",
    "\n",
    "        #Draw Agent\n",
    "        draw.ellipse((self.agent_pos[1]*w + w/4, self.agent_pos[0]*h + h/4, 3*w/4 + self.agent_pos[1]*w, 3*h/4 + self.agent_pos[0]*h), fill=\"black\")\n",
    "\n",
    "        display(image)\n",
    "\n",
    "    def update_state(self,new_pos):\n",
    "      \"\"\"\n",
    "      new_pos - a point in the game grid that the agent has moved to\n",
    "      This function updates the agent position for the GameGrid class variable.\n",
    "      \"\"\"\n",
    "      self.agent_pos = new_pos\n",
    "        \n",
    "    def is_episode_terminal(self):\n",
    "        return True if self.agent_pos == self.terminal_state else False\n",
    "\n",
    "    def refresh_game(self):\n",
    "        self.agent_pos = self.start_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782333a",
   "metadata": {},
   "source": [
    "## Collection Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce3a8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collection_Game(GridWorld):\n",
    "    def __init__(self, board_dims, start_state, prize_states, reward_matrix, prize_value=0):\n",
    "        GridWorld.__init__(self,board_dims, start_state, None, reward_matrix)\n",
    "        self.prize_states = prize_states\n",
    "        self.remaining_prize_states = list(prize_states)\n",
    "        self.prize_value = prize_value\n",
    "        for prize_state in prize_states:\n",
    "            self.reward_matrix[prize_state] = prize_value\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        agent_pos - tuple point representing agent's current position on game board \n",
    "        remaining_prizes - list of tuples representing remaining prizes and their positions on game board\n",
    "        This function draws our environment\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.new(\"RGB\", (501, 501), \"black\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        w = 500/(self.width)\n",
    "        h = 500/(self.height)\n",
    "        color = \"white\"\n",
    "\n",
    "        #Draw Grid and Start/Stop Squares\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if(i == self.start_state[0] and j == self.start_state[1]):\n",
    "                    color = \"blue\"\n",
    "                draw.rectangle(((0+j*w, 0+i*h),(w+j*w, h+i*h)), outline = \"black\", fill = color)\n",
    "                color =\"white\"\n",
    "\n",
    "        #Draw Agent\n",
    "        draw.ellipse((self.agent_pos[1]*w + w/4, self.agent_pos[0]*h + h/4, 3*w/4 + self.agent_pos[1]*w, 3*h/4 + self.agent_pos[0]*h), fill=\"black\")\n",
    "\n",
    "        #Draw Prizes\n",
    "        if len(self.remaining_prize_states) > 1:\n",
    "            for x in self.remaining_prize_states:\n",
    "                draw.rectangle(((x[1]*w + w/4, x[0]*h + h/4), (3*w/4+x[1]*w, 3*h/4+x[0]*h)), outline = \"black\", fill = \"yellow\")\n",
    "        elif len(self.remaining_prize_states) == 1:\n",
    "            remaining_prize = self.remaining_prize_states[0]\n",
    "            draw.rectangle(((remaining_prize[1]*w + w/4, remaining_prize[0]*h + h/4), (3*w/4+remaining_prize[1]*w, 3*h/4+remaining_prize[0]*h)), outline = \"black\", fill = \"yellow\")\n",
    "      \n",
    "        display(image)\n",
    "\n",
    "    def update_state(self,new_pos):\n",
    "        \"\"\"\n",
    "        new_pos - a point in the game grid that the agent has moved to\n",
    "        This function updates the agent position for the GameGrid class variable.\n",
    "        \"\"\"\n",
    "        self.agent_pos = new_pos\n",
    "        if self.agent_pos in self.remaining_prize_states:\n",
    "            self.remove_prize(self.agent_pos)\n",
    "\n",
    "    def remove_prize(self, prize_point):\n",
    "        \"\"\"\n",
    "        prize_point - a point in the game grid that contained a prize\n",
    "        This function removes the prize at the prize_point supplied from the remaining prizes. This function also\n",
    "            updates the reward matrix accordingly.\n",
    "        \"\"\"\n",
    "        #remove prize from remaining prizes\n",
    "        self.remaining_prize_states.remove(prize_point)\n",
    "        #adjust reward matrix to account for no prize at this prize_point for the rest of the episode\n",
    "        self.reward_matrix[prize_point] = self.reward_matrix[self.start_state]\n",
    "        #if there remains one prize, set that to be the terminal state for the episode\n",
    "        if len(self.remaining_prize_states) == 1:\n",
    "            self.terminal_state = self.remaining_prize_states[0]\n",
    "        \n",
    "    def is_episode_terminal(self):\n",
    "        return True if len(self.remaining_prize_states) == 0 else False\n",
    "\n",
    "    def refresh_game(self):\n",
    "        \"\"\"\n",
    "        This function refreshes the game's agent position, the remaining prizes, the reward matrix, and terminal \n",
    "            state. This is used between each episode\n",
    "        \"\"\"\n",
    "        self.agent_pos = self.start_state\n",
    "        self.remaining_prize_states = list(self.prize_states)\n",
    "        self.terminal_state = None\n",
    "        \n",
    "        for prize_state in self.prize_states:\n",
    "            self.reward_matrix[prize_state] = self.prize_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13e12d",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Here we define different agents and allow them to play our games that we define above in the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc2871",
   "metadata": {},
   "source": [
    "## TD Off-Policy Agent: Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bfcbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTable:\n",
    "    \"\"\"\n",
    "    This class implements the qtable object with qlearning update rules\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, alpha, discount):\n",
    "        self.Game = Game\n",
    "        self.matrix = np.zeros([Game.height*Game.width, len(Game.moves)])\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.target_policy = np.ones_like(self.matrix) / len(Game.moves)\n",
    "\n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function gets the deterministic (greedy) policy derived from the qtable\n",
    "        \"\"\"\n",
    "        self.Game.refresh_game()\n",
    "        \n",
    "#         self.Game.agent_pos = self.Game.start_state\n",
    "        move_idxs = np.arange(len(self.Game.moves))\n",
    "        while not self.Game.is_episode_terminal():\n",
    "            next_action_idx = np.argmax(self.matrix[self.Game.point_to_index(self.Game.agent_pos), :])\n",
    "            \n",
    "            self.target_policy[self.Game.point_to_index(self.Game.agent_pos), next_action_idx] = 1\n",
    "            not_next_action_idxs = move_idxs[np.arange(len(move_idxs))!=next_action_idx]\n",
    "        \n",
    "            self.target_policy[self.Game.point_to_index(self.Game.agent_pos),not_next_action_idxs] = 0\n",
    "            \n",
    "            next_action = self.Game.moves[next_action_idx]\n",
    "            new_state = tuple(np.array(self.Game.agent_pos) + np.array(next_action))\n",
    "            self.Game.update_state(new_state)\n",
    "            \n",
    "        self.Game.refresh_game()\n",
    "        \n",
    "    def get_return(self, state, action):\n",
    "        \"\"\"\n",
    "        state - some integer index corresponding to a tuple point in the grid game\n",
    "        action - some integer index corresponding to a possible action taken in the grid game\n",
    "        This function returns the expected return from the qtable for a specific action made in a specific state\n",
    "        \"\"\"\n",
    "        return self.matrix[state, action]\n",
    "\n",
    "    def max_lookahead(self, state):\n",
    "        \"\"\"\n",
    "        state - this is s', a point in the grid where are agent is going to be when taking action a in state s\n",
    "        This function finds the best action which maximizes the next move made from s' (state)\n",
    "        \"\"\"\n",
    "        aprime = np.argmax(self.matrix[state, :])\n",
    "        return self.get_return(state,aprime)\n",
    "\n",
    "    def update_qtable(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        state - a point on the grid where your agent is performing an action from (converted to index in qtable)\n",
    "        action - this is an integer corresponding to a tuple that makes the agent move up, left, down, or right and represents \n",
    "          the selected move made by the agent in the state provided above\n",
    "        (state, action) pair provided above can be plugged directly into qtable\n",
    "        reward - this is an integer corresponding to the reward for the above (state, action) tuple\n",
    "        \"\"\"\n",
    "        # action equals 0, 1, 2, 3\n",
    "        current_reward = self.matrix[state,action]\n",
    "        self.matrix[state, action] = current_reward + self.alpha * (reward + self.discount * self.max_lookahead(new_state) - current_reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee33e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    This class implements a temporal difference off-policy (qlearning) agent that can play grid games\n",
    "    \"\"\"\n",
    "    def __init__(self, Game, model):\n",
    "        self.Game = Game\n",
    "        self.model = model\n",
    "        self.behavior_policy = np.ones_like(self.model.matrix) / len(self.Game.moves)\n",
    "        self.target_policy = None\n",
    "        \n",
    "    def get_target_policy(self):\n",
    "        \"\"\"\n",
    "        This function obtains the deterministic (greedy) policy from the qtable\n",
    "        \"\"\"\n",
    "        self.model.get_target_policy()\n",
    "        self.target_policy = self.model.target_policy\n",
    "    \n",
    "    def update_model(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        state - a tuple point corresponding to a square in a grid game\n",
    "        action - a tuple move corresponding to an action made in a grid game\n",
    "        reward - the reward derived from the game's reward matrix for specific action taken in specific state\n",
    "        new_state - a tuple point corresponding to the square in the grid game that your agent moved to \n",
    "            from taking the action described above in the state described above\n",
    "        This function updates the qtable by calling update_qtable and passing the necessary information about the game environment\n",
    "        \"\"\"\n",
    "        new_state = self.Game.point_to_index(new_state)\n",
    "        state = self.Game.point_to_index(state)\n",
    "        action = self.Game.moves.index(action)\n",
    "        self.model.update_qtable(state, action, reward, new_state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        state - a tuple point that corresponds to a square in a grid game\n",
    "        This function obtains an action from the games possible moves based on the agent's behavior policy\n",
    "        \"\"\"\n",
    "        probs = self.behavior_policy[self.Game.point_to_index(state),:]\n",
    "        r = random.random()\n",
    "        for move_idx, prob in enumerate(probs):\n",
    "            if r > prob:\n",
    "                r -= prob\n",
    "            else:\n",
    "                return self.Game.moves[move_idx]\n",
    "\n",
    "        return self.moves[random.randint(0,len(self.moves))]\n",
    "    \n",
    "    def play_game(self, episodes, output=False):\n",
    "        \"\"\"\n",
    "        episodes - an integer that corresponds to the number of times your agent plays the game\n",
    "        This function has your agent play the game and update its model of the game\n",
    "        \"\"\"\n",
    "        player_scores = []\n",
    "        #number of times player plays the game is episodes.\n",
    "\n",
    "        self.Game.draw()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            #in each episode, the player needs to complete the task in T steps (t = 0,1,...,t-2,t-1,T)\n",
    "\n",
    "            t = 0\n",
    "            episode_reward = 0\n",
    "            \n",
    "\n",
    "            #while agent is not in a terminal state\n",
    "            while not self.Game.is_episode_terminal():\n",
    "                #Take action A, get reward R, step into s'\n",
    "                #Find a that is max(s',a)\n",
    "\n",
    "                if output:\n",
    "                    print(\"Player's position\",self.Game.agent_pos)\n",
    "\n",
    "                t += 1\n",
    "\n",
    "                #get possible moves going up, left, down, and right\n",
    "                new_action = self.get_action(self.Game.agent_pos)\n",
    "                new_state = tuple(np.array(self.Game.agent_pos) + np.array(new_action))\n",
    "\n",
    "                #if move is illegal (going off the board), set reward to very bad\n",
    "                if self.Game.illegal_move(new_state):\n",
    "                    reward = -10000\n",
    "                    self.update_model(self.Game.agent_pos, new_action, reward, self.Game.agent_pos)\n",
    "                else:\n",
    "                #else the selected move is legal and we should get reward r for agent going to state s'\n",
    "                    reward = self.Game.get_reward(new_state)\n",
    "                    self.update_model(self.Game.agent_pos, new_action, reward, new_state)\n",
    "                    self.Game.update_state(new_state)\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                \n",
    "#                 print(\"Player's possible moves: \",self.Game.get_moves(self.Game.agent_pos))\n",
    "#                 print(\"Player's chosen move\",new_state)\n",
    "#                 print(\"Player's new position\",self.Game.agent_pos)\n",
    "#                 print(\"Player's reward for this move\",reward)\n",
    "#                 print(\"Number of moves made to perform task: \",t)\n",
    "\n",
    "                if self.Game.terminal_state is not None and self.Game.agent_pos in self.Game.terminal_state:\n",
    "                    print(\"Player found the target square: \",self.Game.terminal_state)\n",
    "\n",
    "#                 self.Game.draw()\n",
    "                \n",
    "\n",
    "            player_scores.append(episode_reward)\n",
    "\n",
    "            print(\"Player finished task in :\",t, \" moves\")\n",
    "            self.Game.refresh_game()\n",
    "\n",
    "        print(\"Player scores for every episode: \",player_scores)\n",
    "        self.Game.refresh_game()\n",
    "        self.Game.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87670ad",
   "metadata": {},
   "source": [
    "# Game-Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1966c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1.]\n",
      " [-1. -1. -1.]\n",
      " [-1. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "reward_matrix = np.ones((3,3))*-1\n",
    "reward_matrix[(2,2)] = 0\n",
    "print(reward_matrix)\n",
    "target_game = Targeting_Game( (3,3), (0,0), (2,2), reward_matrix)\n",
    "\n",
    "\n",
    "qtable = QLearningTable(target_game, 0.1, 0.25)\n",
    "agent = QLearningAgent(target_game, qtable)\n",
    "\n",
    "#play_game(target_game, 1, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb611cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAH1CAIAAABgKvBGAAAIu0lEQVR4nO3a0YojVxJAwarF///L2oeBhvEYe7yrVmWeG/HcLWVScEgkXRcARfd1Xdf1engKPup+vTzxs9y3h36c+77/8/QMAHwLfQdo0neAJn0HaNJ3gCZ9B2jSd4AmfQdo0neAJn0HaNJ3gCZ9B2jSd4AmfQdo0neAJn0HaNJ3gCZ9B2jSd4AmfQdo0neAJn0HaNJ3gCZ9B2jSd4AmfQdo0neAJn0HaNJ3gCZ9B2jSd4AmfQdo0neAJn0HaNJ3gCZ9B2jSd4AmfQdo0neApj+eHuBj7t/7s9f3TgHwKeG+/2bQ//6/5B7Yqtf3/y3r//hqQg8sU+r7e8v+ly+u8sAajb5/a9l/fSOVBxbY3vePlf3XN1V5YLS9fX+k7L8OoPLAUEt///543L/MmQTgJ+vu94E9dcgDE+263wfG/cvk2YATLer7/IDOnxA4yJa+b0nnljmBvhV93xXNXdMCWfP7vjGXG2cGaob3fW8o904OREzu+/ZEbp8f2G1s3xtxbGwBrDSz76UslnYBNpnZdwD+XwP73jt4exsBC0zrezWF1b2Auab1HYD3GNX39pHb3g4YZ1TfAXibOX0/4bw9YUdgijl9B+CdhvT9nMP2nE2Bhw3pOwBvNqHvp520p+0LPGNC3wF4P30HaHq872d+WHHm1sBHPd53AL6FvgM06TtAk74DND3b95O/Zjx5d+AT3O8ATfoO0KTvAE36DtCk7wBN+g7QpO8ATfoO0KTvAE36DtCk7wBN+g7Q9GzfX4+++7NO3h34BPc7QJO+AzTpO0CTvgM0Pd73M79mPHNr4KMe7zsA30LfAZom9P20DytO2xd4xoS+A/B+Q/p+zkl7zqbAw4b0HYA3m9P3Ew7bE3YEppjTdwDeaVTf2+dteztgnFF9B+BtpvW9euRW9wLmmtb3q5jC3kbAAgP7DsAbzOx76eAt7QJsMrPvVyWLjS2Alcb2/dofx+3zA7tN7vu1OZF7Jwcihvf92hnKjTMDNfP7fm3L5a5pgawVfb/2RHPLnEDflr5fG9I5f0LgIIv6fs0O6OTZgBP98fQA/9aPjN4PT/ETZQcm2nW/f5mT1DmTAPxk3f3+5fFDXtmB0fb2/YdHKq/swALb+/7Dxyqv7MAajb7/8K2VV3ZgmVLff/gK8VtCL+vAVr2+f/lTmn8z94IORIT7/ifCDZxl6e/fAfgH+g7QpO8ATfoO0KTvAE36DtCk7wBN+g7QpO8ATfoO0KTvAE36DtCk7wBN+g7QpO8ATfoO0KTvAE36DtCk7wBN+g7QpO8ATfoO0KTvAE36DtCk7wBN+g7QpO8ATfoO0KTvAE36DtCk7wBN+g7QpO8ATfoO0KTvAE36DtCk7wBN+g7QpO8AALDHfV3X6/V6egw+575vT/w0HvqB7vv2+QxAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7AADscV/X9Xq9nh6Dz7nv2xM/zX3fHvlpbp/PAFTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgM06TtAk74DNOk7QJO+AzTpO0CTvgMAwB7/BaG7Vf9nsoxgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=501x501 at 0x7FEF0EF87FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player finished task in : 15  moves\n",
      "Player finished task in : 23  moves\n",
      "Player finished task in : 61  moves\n",
      "Player finished task in : 27  moves\n",
      "Player finished task in : 64  moves\n",
      "Player finished task in : 28  moves\n",
      "Player finished task in : 18  moves\n",
      "Player finished task in : 50  moves\n",
      "Player finished task in : 10  moves\n",
      "Player finished task in : 56  moves\n",
      "Player finished task in : 4  moves\n",
      "Player finished task in : 7  moves\n",
      "Player finished task in : 17  moves\n",
      "Player finished task in : 40  moves\n",
      "Player finished task in : 34  moves\n",
      "Player finished task in : 21  moves\n",
      "Player finished task in : 14  moves\n",
      "Player finished task in : 14  moves\n",
      "Player finished task in : 6  moves\n",
      "Player finished task in : 8  moves\n",
      "Player finished task in : 26  moves\n",
      "Player finished task in : 32  moves\n",
      "Player finished task in : 5  moves\n",
      "Player finished task in : 20  moves\n",
      "Player finished task in : 18  moves\n",
      "Player finished task in : 6  moves\n",
      "Player finished task in : 109  moves\n",
      "Player finished task in : 10  moves\n",
      "Player finished task in : 17  moves\n",
      "Player finished task in : 6  moves\n",
      "Player finished task in : 57  moves\n",
      "Player finished task in : 13  moves\n",
      "Player finished task in : 80  moves\n",
      "Player finished task in : 14  moves\n",
      "Player finished task in : 18  moves\n",
      "Player finished task in : 30  moves\n",
      "Player finished task in : 5  moves\n",
      "Player finished task in : 4  moves\n",
      "Player finished task in : 7  moves\n",
      "Player finished task in : 15  moves\n",
      "Player finished task in : 9  moves\n",
      "Player finished task in : 13  moves\n",
      "Player finished task in : 83  moves\n",
      "Player finished task in : 18  moves\n",
      "Player finished task in : 19  moves\n",
      "Player finished task in : 23  moves\n",
      "Player finished task in : 29  moves\n",
      "Player finished task in : 12  moves\n",
      "Player finished task in : 5  moves\n",
      "Player finished task in : 65  moves\n",
      "Player scores for every episode:  [-70007.0, -90013.0, -110049.0, -130013.0, -240039.0, -120015.0, -60011.0, -220027.0, -20007.0, -160039.0, -3.0, -30003.0, -70009.0, -180021.0, -100023.0, -50015.0, -40009.0, -20011.0, -20003.0, -7.0, -60019.0, -140017.0, -10003.0, -60013.0, -60011.0, -5.0, -370071.0, -40005.0, -50011.0, -5.0, -230033.0, -50007.0, -300049.0, -60007.0, -20015.0, -120017.0, -10003.0, -3.0, -30003.0, -30011.0, -30005.0, -30009.0, -250057.0, -40013.0, -110007.0, -50017.0, -90019.0, -40007.0, -10003.0, -230041.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=501x501 at 0x7FEF0EF87FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.play_game(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b18a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9992.58312911 -9999.47413326    -1.29694391    -1.29628554]\n",
      " [-9948.75232146    -1.3027852     -1.22240498    -1.21867972]\n",
      " [-9656.83435716    -1.25495432    -0.95760884 -9722.08576926]\n",
      " [   -1.30669374 -9774.98114153    -1.2164376     -1.21823909]\n",
      " [   -1.27540132    -1.23791034    -0.97496844    -0.95760884]\n",
      " [   -0.99952803    -1.08120536     0.         -7941.08867905]\n",
      " [   -1.22362314 -9576.2995699  -9867.19523121    -0.98922474]\n",
      " [   -1.0465753     -1.17543513 -9576.08841725     0.        ]\n",
      " [    0.             0.             0.             0.        ]]\n",
      "[[0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(agent.model.matrix)\n",
    "agent.get_target_policy()\n",
    "print(agent.target_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d848bc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=501x501 at 0x7FEF0EF87D30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player finished task in : 19  moves\n",
      "Player finished task in : 4  moves\n",
      "Player finished task in : 27  moves\n",
      "Player finished task in : 38  moves\n",
      "Player finished task in : 93  moves\n",
      "Player finished task in : 13  moves\n",
      "Player finished task in : 135  moves\n",
      "Player finished task in : 29  moves\n",
      "Player finished task in : 14  moves\n",
      "Player finished task in : 6  moves\n",
      "Player finished task in : 44  moves\n",
      "Player finished task in : 10  moves\n",
      "Player finished task in : 33  moves\n",
      "Player finished task in : 70  moves\n",
      "Player finished task in : 30  moves\n",
      "Player finished task in : 72  moves\n",
      "Player finished task in : 32  moves\n",
      "Player finished task in : 29  moves\n",
      "Player finished task in : 29  moves\n",
      "Player finished task in : 13  moves\n",
      "Player finished task in : 22  moves\n",
      "Player finished task in : 38  moves\n",
      "Player finished task in : 18  moves\n",
      "Player finished task in : 49  moves\n",
      "Player finished task in : 15  moves\n",
      "Player finished task in : 9  moves\n",
      "Player finished task in : 28  moves\n",
      "Player finished task in : 33  moves\n",
      "Player finished task in : 31  moves\n",
      "Player finished task in : 33  moves\n",
      "Player finished task in : 29  moves\n",
      "Player finished task in : 28  moves\n",
      "Player finished task in : 27  moves\n",
      "Player finished task in : 90  moves\n",
      "Player finished task in : 30  moves\n",
      "Player finished task in : 18  moves\n",
      "Player finished task in : 13  moves\n",
      "Player finished task in : 8  moves\n",
      "Player finished task in : 8  moves\n",
      "Player finished task in : 33  moves\n",
      "Player finished task in : 19  moves\n",
      "Player finished task in : 13  moves\n",
      "Player finished task in : 30  moves\n",
      "Player finished task in : 39  moves\n",
      "Player finished task in : 50  moves\n",
      "Player finished task in : 36  moves\n",
      "Player finished task in : 33  moves\n",
      "Player finished task in : 96  moves\n",
      "Player finished task in : 41  moves\n",
      "Player finished task in : 17  moves\n",
      "Player scores for every episode:  [-110006.0, -2.0, -70018.0, -120024.0, -210070.0, -30008.0, -510082.0, -90018.0, -60006.0, -4.0, -140028.0, -20006.0, -70024.0, -260042.0, -80020.0, -280042.0, -140016.0, -50022.0, -50022.0, -10010.0, -80012.0, -200016.0, -20014.0, -130034.0, -50008.0, -10006.0, -140012.0, -130018.0, -90020.0, -110020.0, -110016.0, -80018.0, -70018.0, -400048.0, -80020.0, -40012.0, -30008.0, -40002.0, -20004.0, -90022.0, -30014.0, -30008.0, -80020.0, -110026.0, -220026.0, -140020.0, -70024.0, -260068.0, -110028.0, -50010.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=501x501 at 0x7FEF0EF91CA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_matrix = np.ones((3,3))*-1\n",
    "reward_matrix[(1,1)] = 0\n",
    "reward_matrix[(2,2)] = 0\n",
    "\n",
    "collection_game = Collection_Game( (3,3), (0,0), [(1,1),(2,2)],reward_matrix)\n",
    "\n",
    "qtable = QLearningTable(collection_game, 0.1, 0.25)\n",
    "agent = QLearningAgent(collection_game, qtable)\n",
    "\n",
    "agent.play_game(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb569ab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9999.45343299 -9997.87148288    -1.23613942    -1.20661677]\n",
      " [-9958.46975877    -1.29742469    -1.10811835    -1.24690967]\n",
      " [-9982.27577385    -1.21588887    -1.00792706 -9975.59279361]\n",
      " [   -1.29454863 -9966.40963751    -1.25032055    -0.98335846]\n",
      " [   -1.21798047    -1.22332864    -1.02129595    -1.01634788]\n",
      " [   -1.21548283    -0.94304948    -0.11526232 -8649.1681305 ]\n",
      " [   -1.22620218 -9880.52280302 -9852.43888891    -1.02259155]\n",
      " [   -1.1068644     -1.20238859 -9656.66275952    -0.08707601]\n",
      " [   -0.27296252    -0.4930096  -2710.01296572 -4095.11296572]]\n",
      "[[0.   0.   0.   1.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(agent.model.matrix)\n",
    "agent.get_target_policy()\n",
    "print(agent.target_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd4f7c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [(-1,0),(0,-1),(1,0),(0,1)]\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf1a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8c715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
